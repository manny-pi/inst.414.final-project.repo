{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject: Re: Christian Daemons? [Biblical Demons, the u\\nFrom: stigaard@mhd.moorhead.msus.edu\\nReply-To: stigaard@mhd.moorhead.msus.edu\\nOrganization: Moorhead State University, Moorhead, MN\\nNntp-Posting-Host: 134.29.97.2\\nLines: 23\\n\\n>>>667\\n>>>the neighbor of the beast\\n>>\\n>>No, 667 is across the street from the beast.  664 and 668 are the\\n>>neighbors of the beast.\\n>\\n>I think some people are still not clear on this:\\n>667 is *not* the neighbor of the beast, but, rather, across the\\n>street. It is, in fact, 668 which is the neighbor of the beast.\\n\\nno, sheesh, didn\\'t you know 666 is the beast\\'s apartment?  667 is across the\\nhall from the beast, and is his neighbor along with the rest of the 6th floor.\\n\\n>Justin (still trying to figure out what this has to do with alt.discordia)\\n\\nThis doesn\\'t seem discordant to you?\\n\\n-----------------------     ----------------------     -----------------------\\n\\t-Paul W. Stigaard, Lokean Discordian Libertarian\\n  !XOA!\\t\\tinternet:  stigaard@mhd1.moorhead.msus.edu\\n (fnord)       Episkopos and Chair, Moorhead State University Campus Discordians\\n\\t\\tRectal neufotomist at large\\n     \"If I left a quote here, someone would think it meant something.\"\\n',\n",
       " \"From: matmcinn@nuscc.nus.sg (Matthew MacIntyre at the National University of Senegal)\\nSubject: Re: WARNING.....(please read)...\\nOrganization: National University of Singapore\\nX-Newsreader: Tin 1.1 PL4\\nLines: 9\\n\\ncallison@uokmax.ecn.uoknor.edu (James P. Callison) writes:\\n: >> \\n: >I'm not going to argue the issue of carrying weapons, but I would ask you if \\n: >you would have thought seriously about shooting a kid for setting off your\\n: >alarm?  I can think of worse things in the world.  Glad you got out of there\\n: >before they did anything to give you a reason to fire your gun.\\n: \\nI think people have a right to kill to defend their property. Why not? Be\\nhonest: do you really care more about scum than about your  car?\\n\",\n",
       " 'From: boyle@cactus.org (Craig Boyle)\\nSubject: Re: Did US drive on the left?\\nArticle-I.D.: cactus.1993Apr6.060553.22453\\nOrganization: Capital Area Central Texas UNIX Society, Austin, Tx\\nLines: 14\\n\\nIn article <YfkBJQS00Uh_E9TFo_@andrew.cmu.edu> \"Daniel U. Holbrook\" <dh3q+@andrew.cmu.edu> writes:\\n>>>\\n>\\n[stuff about RHD deSoto\\'s deleted]\\n\\n>Well Sweden and Australia, and lord knows wherever else used to drive on\\nAustralians still do drive on the \"wrong\" side of the road. I believe\\nSweden changed in 1968. The way I heard it was that they swapped\\nall the traffic signs around one Sunday....\\n\\n>the \"wrong\" side of the road, so the export market might have been\\n>larger then than just the UK.\\n>\\nCraig\\n',\n",
       " 'From: afhetzel@netcom.com (A.F. Hetzel)\\nSubject: Aviation Headset D.C. H10-40 For Sale\\nOrganization: Netcom Online Communications Services (408-241-9760 login: guest)\\nLines: 25\\n\\n\\nFor Sale:\\n\\nDavid Clark H10-40 Aviation Headset\\n\\nExcellent Condition (not even a scratch) -- original packaging.\\n\\n     Discover for yourself why the H10-40 continues to be the favorite headset\\nof thousands of pilots.  It was the first headset to have the advanced M-4\\namplified electret microphone - with a frequency response specifically\\ndesigned to match the human voice.  Also includes durable universal boom\\nassembly and a noise reduction rating (NRR) of 24dB.  Weighs 19 oz.\\n\\n** Includes Telex \"push to talk switch\"\\n\\nAsking $220.00 U.S.\\n\\nShipping negotiable.     \\n\\nFor more information respond to: afhetzel@netcom.com (Andrew)\\n\\n-- \\n Andrew F. Hetzel            \"I complete less work before 9:00am than  \\n afhetzel@netcom.com          most people do all day.\"              \\n Ann Arbor, MI USA                          \\n',\n",
       " 'From: Nanci Ann Miller <nm0w+@andrew.cmu.edu>\\nSubject: Re: It\\'s all Mary\\'s fault!\\nOrganization: Sponsored account, School of Computer Science, Carnegie Mellon, Pittsburgh, PA\\nLines: 28\\n\\t<C5KEqu.4xo@portal.hq.videocart.com>\\nNNTP-Posting-Host: po5.andrew.cmu.edu\\nIn-Reply-To: <C5KEqu.4xo@portal.hq.videocart.com>\\n\\ndfuller@portal.hq.videocart.com (Dave Fuller) writes:\\n>   Nice attempt Chris . . . verrry close.\\n> \\n>   You missed the conspiracy by 1 step. Joseph knew who knocked her up.\\n> He couldn\\'t let it be known that somebody ELSE got ol\\' Mary prego. That\\n> wouldn\\'t do well for his popularity in the local circles. So what \\n> happened is that she was feeling guilty, he was feeling embarrassed, and\\n> THEY decided to improve both of their images on what could have otherwise\\n> been the downfall for both. Clever indeed. Come to think of it . . . I\\n> have gained a new respect for the couple. Maybe Joseph and Mary should\\n> receive all of the praise being paid to jesus.\\n\\nLucky for them that the baby didn\\'t have any obvious deformities!  I could\\njust see it now: Mary gets pregnant out of wedlock so to save face she and\\nJoseph say that it was God that got her pregnant and then the baby turns\\nout to be deformed, or even worse, stillborn!  They\\'d have a lot of\\nexplaining to do.... :-)\\n\\n> Dave \"Buckminster\" Fuller\\n> How is that one \\'o keeper of the nicknames ?\\n\\nNanci\\n.........................................................................\\nIf you know (and are SURE of) the author of this quote, please send me\\nemail (nm0w+@andrew.cmu.edu):\\nLife does not cease to be funny when people die, any more than it ceases to\\nbe serious when people laugh.\\n\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.space\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "print(\"Loading 20 newsgroups training data\")\n",
    "raw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\n",
    "data_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\n",
    "print(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'simple', 'example', 'isn', 't', 'it']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(doc): \n",
    "    \"\"\"Extract tokens from doc. \n",
    "\n",
    "    This uses a simple regex that matches word characters to break strings\n",
    "    into tokens. For a more principled approach, see CountVectorizer or TfidfVectorizer.\n",
    "    \"\"\"\n",
    "\n",
    "    return [tok.lower() for tok in re.findall(r\"\\w+\", doc)]\n",
    "\n",
    "list(tokenize(\"This is a simple example, isn't it?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'that': 1,\n",
       "             'is': 2,\n",
       "             'one': 1,\n",
       "             'example': 1,\n",
       "             'but': 1,\n",
       "             'this': 1,\n",
       "             'another': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_freqs(doc): \n",
    "    \"\"\"Extract a dict mapping tokens from doc to their occurrences.\"\"\" \n",
    "\n",
    "    freq = defaultdict(int)\n",
    "    for tok in tokenize(doc): \n",
    "        freq[tok] += 1\n",
    "    return freq\n",
    "\n",
    "token_freqs(\"That is one example, but this is another\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.053s at 5.9 MB/s\n",
      "Found 47928 unique terms\n"
     ]
    }
   ],
   "source": [
    "# track the performance of the different vectorizers\n",
    "dict_count_vectorizers = defaultdict(list)\n",
    "\n",
    "t0 = time() \n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0 \n",
    "\n",
    "dict_count_vectorizers[\"vectorizer\"].append(\n",
    "    vectorizer.__class__.__name__ + \"\\non freq dicts\"\n",
    ")\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f}s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DictVectorizer\\non freq dicts']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count_vectorizers[\"vectorizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('appease', 8590)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.items())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
